# ROC-Analyse und formale Metriken

## Ãœbersicht

Dieses Projekt nutzt jetzt professionelle Signalverarbeitungs-Metriken:

- **ROC-Kurven** (Receiver Operating Characteristic)
- **AUC** (Area Under Curve) - Threshold-unabhÃ¤ngiges QualitÃ¤tsmaÃŸ
- **Precision-Recall Kurven** - FÃ¼r unbalancierte DatensÃ¤tze
- **Score Distributions** - Visualisierung der Modell-Konfidenz

## ğŸ¯ Warum ROC-Kurven?

### Problem mit festen Thresholds:
```python
# Alter Ansatz: Ein fester Threshold (z.B. 0.75)
prediction = model.predict(data) > 0.75
```

**Nachteile:**
- Ergebnis hÃ¤ngt stark von Threshold-Wahl ab
- Unfairer Vergleich zwischen Modellen
- Keine Aussage Ã¼ber optimalen Arbeitspunkt

### LÃ¶sung: ROC-Kurve
Die ROC-Kurve zeigt **alle mÃ¶glichen Thresholds gleichzeitig**:

```
TPR (True Positive Rate)  = Wie viele Signale finden wir?
    ^
    |     /----  Perfektes Modell (AUC=1.0)
1.0 |    /
    |   /
    |  /  <- Unser Modell (AUC=0.85)
0.5 |/
    |  /  <- Zufall (AUC=0.5)
    |/_________________>
    0                 1.0  FPR (False Positive Rate)
                           Wie oft liegen wir falsch?
```

**ROC AUC Interpretation:**
- `AUC = 1.0`: Perfekt! Findet alle Signale ohne Fehler
- `AUC = 0.9`: Exzellent (LIGO-Niveau)
- `AUC = 0.8`: Gut
- `AUC = 0.5`: Nicht besser als Zufall

## ğŸ“Š Neue Scripts

### 1. `evaluate_model_with_roc.py` - Erweiterte Einzelmodell-Analyse

**Was es macht:**
- Erstellt ROC-Kurve fÃ¼r ein Modell
- Berechnet AUC und Average Precision
- Zeigt Operating Point (gewÃ¤hlter Threshold)
- 6 verschiedene Visualisierungen

**Usage:**
```bash
python evaluate_model_with_roc.py
```

**Output:**
```
ğŸ¤– COMPREHENSIVE EVALUATION: gwd_model_20250123-143022.keras
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š FINAL SUMMARY:
  ROC AUC:              0.9234
  Average Precision:    0.9156
  Accuracy:             87.3%
  TPR (Sensitivity):    91.2%
  FPR:                  16.5%
  SNR90:                1.45
  Real Events Found:    4/5
```

**6 Plots:**
1. **ROC-Kurve** - Threshold-unabhÃ¤ngige Performance
2. **Precision-Recall** - Alternative Darstellung
3. **Confusion Matrix** - Bei gewÃ¤hltem Threshold
4. **Sensitivity Curve** - SNR vs Detection Rate
5. **Score Distribution** - Wie trennt das Modell Signal/Noise?
6. **Metriken-Tabelle** - Alle Zahlen auf einen Blick

### 2. `compare_models_roc.py` - Modell-Vergleich

**Was es macht:**
- Vergleicht ALLE Modelle in einer ROC-Kurve
- Ranking nach AUC
- Zoom auf relevante Region (FPR < 0.1)

**Usage:**
```bash
python compare_models_roc.py
```

**Output:**
```
ğŸ† ROC CURVE COMPARISON TOOL

âœ“ Geladen: gwd_model_20250120-120000
âœ“ Geladen: gwd_model_20250121-140000
âœ“ Geladen: gwd_model_20250123-160000

ğŸ“Š FINAL RANKING:
  1. gwd_model_20250123-160000  | AUC: 0.9456
  2. gwd_model_20250121-140000  | AUC: 0.9123
  3. gwd_model_20250120-120000  | AUC: 0.8845
```

## ğŸ”¬ Wissenschaftliche Metriken erklÃ¤rt

### ROC AUC (Area Under Curve)
```python
# Interpretation:
AUC = P(score(signal) > score(noise))
```
**Bedeutung:** Wahrscheinlichkeit, dass ein zufÃ¤lliges Signal einen hÃ¶heren Score bekommt als zufÃ¤lliges Rauschen.

### Average Precision (AP)
```python
# Wichtig bei unbalancierten Daten
AP = Durchschnitt aller Precision-Werte Ã¼ber alle Recall-Level
```
**Vorteil:** Bestraft False Positives hÃ¤rter als ROC AUC

### TPR (True Positive Rate) = Recall = Sensitivity
```python
TPR = TP / (TP + FN)
```
**Bedeutung:** Von allen echten Signalen - wie viele finden wir?

### FPR (False Positive Rate)
```python
FPR = FP / (FP + TN)
```
**Bedeutung:** Von allen Rausch-Segmenten - wie oft schlagen wir Fehlalarm?

## ğŸ® Praktische Anwendung

### Beispiel 1: Threshold-Optimierung

Du siehst in der ROC-Kurve:
- Bei `Threshold = 0.5`: TPR=95%, FPR=30% â†’ Viele Fehlalarme
- Bei `Threshold = 0.75`: TPR=85%, FPR=10% â†’ Ausgewogen
- Bei `Threshold = 0.9`: TPR=60%, FPR=2% â†’ Konservativ

**Wahl hÃ¤ngt ab von:**
- Wissenschaftliche Analyse â†’ Niedriger FPR wichtiger (Threshold hoch)
- Trigger fÃ¼r Folge-Analysen â†’ Hoher TPR wichtiger (Threshold niedrig)

### Beispiel 2: Modell-Vergleich

```
Modell A: AUC = 0.92, SNR90 = 1.8
Modell B: AUC = 0.88, SNR90 = 1.5

â†’ Modell A ist threshold-unabhÃ¤ngig besser (hÃ¶herer AUC)
â†’ ABER: Modell B ist sensitiver bei niedrigem SNR!
```

**Lesson:** AUC allein reicht nicht, schaue auch auf Sensitivity!

## ğŸ“ˆ Verbesserungen im Leaderboard

Die CSV enthÃ¤lt jetzt zusÃ¤tzlich:

| Column | Bedeutung |
|--------|-----------|
| `ROC_AUC` | Threshold-unabhÃ¤ngiges QualitÃ¤tsmaÃŸ |
| `Avg_Precision` | PrÃ¤zision Ã¼ber alle Recall-Level |
| `TPR` | True Positive Rate bei gewÃ¤hltem Threshold |
| `FPR` | False Positive Rate bei gewÃ¤hltem Threshold |

**Alter Leaderboard:**
```csv
Model,Sim_Accuracy,Sim_SNR90
model_1,85%,1.8
model_2,87%,1.5
```

**Neuer Leaderboard:**
```csv
Model,ROC_AUC,Avg_Precision,TPR,FPR,Sim_SNR90,Physik_Gap
model_1,0.9234,0.9156,91%,16%,1.8,+0.30
model_2,0.8945,0.8823,88%,12%,1.5,+0.00
```

## ğŸ”§ Integration in Workflow

### Kompletter Evaluations-Workflow:

```bash
# 1. Modell trainieren
python dataset/train_cnn.py

# 2. Physik-Baseline berechnen (einmalig)
python evaluate_classic.py

# 3. Modell evaluieren (mit ROC)
python evaluate_model_with_roc.py

# 4. Alle Modelle vergleichen
python compare_models_roc.py

# 5. Leaderboard anschauen
python show_leaderboard.py
```

## ğŸ“š WeiterfÃ¼hrende Literatur

**ROC-Kurven in GW-Physik:**
- LIGO Scientific Collaboration Papers verwenden immer ROC/AUC
- Standard bei Machine Learning fÃ¼r GW-Detection
- Vergleichbar mit "Efficiency Curves" in Particle Physics

**Typische Werte in der Literatur:**
- LIGO Matched Filter: AUC â‰ˆ 0.95-0.98 (Physik-Limit)
- Deep Learning Modelle: AUC â‰ˆ 0.90-0.95
- Naive Methoden: AUC â‰ˆ 0.70-0.80

## âš ï¸ HÃ¤ufige Fehler

### âŒ Fehler 1: AUC auf Trainings-Daten
```python
# FALSCH:
y_pred = model.predict(X_train)
auc = roc_auc_score(y_train, y_pred)  # Zu optimistisch!
```

**Fix:** Immer separate Test-Daten verwenden!

### âŒ Fehler 2: Unbalancierte Daten ignorieren
```python
# Bei 90% Rauschen, 10% Signale:
# Accuracy = 90% klingt gut, aber Modell findet kein Signal!
```

**Fix:** Schaue auch auf Precision-Recall Kurve!

### âŒ Fehler 3: Threshold nach ROC festlegen
```python
# ROC zeigt nur was MÃ–GLICH ist
# Den Threshold musst du basierend auf deiner Anwendung wÃ¤hlen!
```

## ğŸ¯ Zusammenfassung

**Was du jetzt hast:**
âœ… Professionelle ROC-Analyse wie in Papers
âœ… Threshold-unabhÃ¤ngiger Modell-Vergleich
âœ… Mehrere komplementÃ¤re Metriken
âœ… Visualisierung der TrennschÃ¤rfe
âœ… Wissenschaftlich fundierte Evaluation

**NÃ¤chste Schritte:**
1. Trainiere mehrere Modelle mit verschiedenen Architekturen
2. Vergleiche sie mit `compare_models_roc.py`
3. WÃ¤hle das beste Modell basierend auf AUC UND deiner Anwendung
4. Optimiere den Threshold basierend auf ROC-Kurve

**Pro-Tipp fÃ¼r Paper/PrÃ¤sentation:**
Zeige immer die ROC-Kurve! Sie ist der Standard in der Community und zeigt, dass du weiÃŸt, was du tust. ğŸš€
